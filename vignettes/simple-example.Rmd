---
title: "A Simple Example"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{A Simple Example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

[MMF: Shall we add a little intro? "In this simple example,... "]

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(bcf2)
library(ggplot2)
```

## Simulate data

First, we simulate some data for testing.  This data set has three covariates $X$, which we divide into two control variables -- that is, two covariates related to the outcome, $y$ -- and one effect moderator -- that is, a covariates related to the treatment effect, $\tau$.

We draw three random $X$s for each unit and generate each unit's outcome, $\mu_i$, [MMF: let's save the word "outcome" for referring to $y$. What does Jared refer to $\mu$ as in the paper? It would be nice to clearly differentiate between $y$ vs. $\mu$ both here in the text, and also down below in the code.] as a function of $x^{(1)}_i$ and $x^{(2)}_i$.  Each unit's probability of joining the intervention, $\pi_i$, is also a function of $\mu_i$, so that units with larger outcomes are more likely to participate in the intervention.  We then assign units to treatment ($z_i = 1$) or comparison ($z_i = 0$) as a function of $\pi_i$.

Then we generate the true treatment effect for each unit, $\tau_i$.  As noted above, $\tau_i$ is a function of $x^{(3)}_i$.  The observed outcome, $y_i$, is a function of $\mu_i$, $\tau_i$, and a random error term $\sigma$. [MMF: and weights?]

```{r data creation}
set.seed(1)

p <- 3 # two control variables and one effect moderator
n <- 20 [MMF: Shall we increase `n` a bit to make this a more interesting example?]
n_burn <- 100
n_sim <- 150


x <- matrix(rnorm(n*p), nrow=n)

weights <- 1.0*rep(1, n) # [Shall we allow these to vary? Shall we allow them to affect the variance of y?]


# create targeted selection, whereby a unit's likelihood of joining the intervention (pi) is related to its expected outcome (mu)
q <- -1*(x[,1]>(x[,2])) + 1*(x[,1]<(x[,2])) -0.1

# generate treatment variable
pi <- pnorm(q)
z <- rbinom(n,1,pi)

# tau is the true treatment effect. It varies across units as a function of
# X3, the effect moderator
tau <- 1/(1 + exp(-x[,3]))

# generate the response using q, tau and z
mu <- (q + tau*z)

# set the noise level relative to the expected mean function of Y
sigma <- diff(range(q + tau*pi))/8

# draw the response variable with additive error
y <- mu + sigma*rnorm(n)
```

## Fit BCF model

In this data set we have observed $y_i$, $x_i$, and $\pi_i$ values to which we can fit our BCF model.  We can then compare the $\hat{\tau}_i$ estimates from BCF to the true $\tau_i$ from the data-generating process.  Note that we are using the `n_chains` argument to `bcf()`, which allows us to run several MCMC chains in parallel and assess whether they have converged to the posterior distribution.

```{r model}
bcf_out <- bcf2::bcf(y            = y,
                 z                = z,
                 x_control        = x,
                 x_moderate       = x,
                 pihat            = pi,
                 nburn            = n_burn,
                 nsim             = n_sim,
                 w                = weights,
                 n_chains         = 4,
                 n_chain_clusters = 2, # [MMF: do we clarify somewhere what this argument does?]
                 random_seed      = 1,
                 update_interval  = 1) # [MMF: and this argument too?]

cat("BCF run complete\n") #[MMF: cut?]
```

## Check MCMC diagnostics

[MMF: Let's (a) add some text around these checks and (b) increase `n_sim` and `n_burn` so that we pass the diagnostic checks.]

```{r diagnostics}
bcf2::summarise_bcf(bcf_out)
```

## Explore the posterior

Now that we've successfully [MMF: ?] fit our model, let's expore the output.  First, since this is a simulation, let's compare the unit-speciifc treatment effect estimates $\hat{\tau}_i$ to the true unit-specific treatment effects $\tau_i$.


### Estimates vs. true treatment effects

```{r}
ggplot(NULL, aes(x = tau, y = colMeans(bcf_out$tau))) +
  geom_abline() +
  geom_point() +
  xlab("True unit-specific treatment effect (tau_i)") + # [MMF: can we TeX tau_i?]
  ylab("Estimate (posterior mean) of unit-specific treatment effect (tau\hat_i)") # [MMF: can we TeX tau\hat_i?]
```

Clearly, BCF correctly recovers the true treatment effects in this simple example. [MMF: hm... this isn't so clear to me. That's a pretty loose scatter. Hopefully it'll tighten up as (a) we increase $n$ and (b) we increase `n_burn` and `n_sim`.] We can also calculate how well the BCF treatment effect estimates explain variation in the true treatment effects. [MMF: I think the scatterplot suffices. OK to cut this sentence and the rest of this section?]
```{r}
tau_fit <- lm(tau ~ colMeans(bcf_out$tau))
coef <- summary(tau_fit)$coef[2,"Estimate"]

## LVF added code to extract the adjusted R2
adjR2 <- summary(tau_fit)$adj.r.squared
```
Our estimates $\hat{\tau}_i$ explain `r round(adjR2, 2)*100`% of the variation in the $\tau_i$, reaffirming the idea that BCF provides accurate treatment effect estimates.

### Distribution of treatment effects

Now that we have confidence in BCF's estimates of unit-specific treatment effects, we examine the results more closely.  First we consider the distribution of unit-specific treatment effects, to assess treatment effect heterogeneity across units.  This plot shows the impact estimate and 95 percent credible interval for each unit, arranged by the size of the impact.

```{r, echo = FALSE, eval = FALSE}
ggplot(NULL, aes(x = 1:(4*n_sim), y = bcf_out$tau[,1])) +
  geom_line() +
  ylab("tau estimate for unit 1") +
  xlab("draws")
```

```{r}
tau_ests <- data.frame(Mean  = colMeans(bcf_out$tau),
                       Low95 = apply(bcf_out$tau, 2, function(x) quantile(x, 0.025)),
                       Up95  = apply(bcf_out$tau, 2, function(x) quantile(x, 0.975)))

tau_ests <- tau_ests[order(tau_ests$Mean), ]
tau_ests[, "id"] <- 1:ncol(bcf_out$tau)

ggplot(tau_ests, aes(x = id, y = Mean)) +
  theme_bw() +
  geom_pointrange(aes(ymin = Low95, ymax = Up95), color = "blue") +
  geom_vline(xintercept = 0, color = "black") +
  ylab("Estimate of unit-specific treatment effect (posterior mean and 95% CI)") +
  xlab("Unit") +
  coord_flip()
```

In this example, although we do see variation in the point estimates across units, the uncertainty intervals all overlap.  [MMF: OK to cut the next sentence? In actual applications, results will vary depending on the application!] In actual applications, the differences among units are likely to be more pronounced and to suggest demarcations between groups of high-performers and low-performers.

### Identifying possible subgroups

The BCF model obtains more accurate treatment effect estimates than other approaches in part by allowing flexible relationships with many possible effect modifiers.  These relationships are often of substantive interest; researchers want to know what covariates are associated with higher or lower impacts.  However, the BCF model is difficult to interpret directly: we cannot simply look at the coefficients on the effect modifiers to determine which characteristics drive the impacts.  Instead, we can look for patterns in the unit-specific treatment effects, linking each unit's treatment effect estimate to its covariates.

Specifically, Hill (2010) [MMF: Which paper is this?] recommends fitting a CART model where the response variable is the estimated unit-specific treatment effect from BCF and the predictor variables are the effect modifiers used to fit the BCF model.  The resulting tree identifies covariates that best distinguish units with high vs. low impacts, which are strong candidate subgroup variables.  However, because the CART fit does not account for the uncertainty in the treatment effect estimates, it is possible that its tree splits do not identify characteristics that create subgroups with meaningfully different impacts.  To conduct inference on whether impacts differ meaningfully based on a characteristic, we can use the posterior draws from the fitted BCF model.

Here we skip [MMF: why?] the CART fit step and show just a comparison of the impacts for units in the top quartile of $X_3$ vs. impacts for units in the bottom quartile of $X_3$, which we know to be a strong effect modifier because we specified it as such in the data generation step.

```{r}
q1 <- x[,3] < quantile(x[,3], 0.25) # [MMF: Thoughts on tidyverse-ing this code chunk?]
q4 <- x[,3] > quantile(x[,3], 0.75)

q1Taus <- bcf_out$tau[,q1]
q4Taus <- bcf_out$tau[,q4]

wq1Taus <- apply(q1Taus, 1, weighted.mean, weights[q1])
wq4Taus <- apply(q4Taus, 1, weighted.mean, weights[q4])

groupTaus <- data.frame(taus     = c(wq1Taus, wq4Taus),
                        quartile = c(rep("q1", 4*n_sim), rep("q4", 4*n_sim)))

ggplot(groupTaus, aes(taus, fill = quartile, color = quartile)) +
  geom_density(alpha = 0.5)
```

[MMF: say what's being shown here.]

As anticipated based on the overlap across the credible intervals of the unit-specific treatment effects, this plot indicates that the posterior distributions for the impact in these two subgroups overlap considerably.  We would therefore not conclude that $X_3$ is an important subgroup variable. [MMF: hopefully, if we increase $n$, these will separate out. Report P(tau_q4 > tau_q1)?]
